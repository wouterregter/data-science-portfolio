{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "promising-obligation",
   "metadata": {},
   "source": [
    "# Kaggle: Customer Transaction Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-manchester",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-richardson",
   "metadata": {},
   "source": [
    "First, I import the necessary models and the train and test datasets from Kaggle. Throughout the feature engineering process I run a baseline logistic regression model to check the change in baseline perfomance after each large change in feature engineering, I might therefore reference to this throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-japan",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "australian-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-juvenile",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nasty-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-cincinnati",
   "metadata": {},
   "source": [
    "### Inspecting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-mississippi",
   "metadata": {},
   "source": [
    "We will print the heads of both the train and test sets to see what they consist of. The train and test datasets each contain 200k records with 200 features. Only the train dataset contains the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aware-translator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 202)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pleasant-anger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 201)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   \n",
       "1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   \n",
       "2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   \n",
       "3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   \n",
       "4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   \n",
       "\n",
       "     var_7   var_8  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.2675  2.1337  ...  -2.1556  11.8495  -1.4300   2.4508  13.7112   2.4669   \n",
       "1  18.6316 -4.4131  ...  10.6165   8.8349   0.9403  10.1282  15.5765   0.4773   \n",
       "2  20.2537  1.5233  ...  -0.7484  10.9935   1.9803   2.1800  12.9813   2.1281   \n",
       "3  20.5660  3.3755  ...   9.5702   9.0766   1.6580   3.5813  15.1874   3.1656   \n",
       "4  10.6048  2.9890  ...   4.2259   9.1723   1.2835   3.3778  19.5542  -0.2860   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   4.3654  10.7200  15.4722  -8.7197  \n",
       "1  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-project",
   "metadata": {},
   "source": [
    "### Inspecting the Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-stack",
   "metadata": {},
   "source": [
    "We will now plot the distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "experimental-copyright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-astrology",
   "metadata": {},
   "source": [
    "The classes are highly imbalanced so we might probably benefit from doing something about that. I tried oversampling the data using SMOTE but it significantly reduced the accuracy of the baseline model. Hence, we will leave the class imbalance like this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incident-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To deal with the class imbalance of the target variable we will use SMOTE, a method of oversampling the data.\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#smote = SMOTE() \n",
    "#train, y = smote.fit_resample(train.drop([\"ID_code\", \"target\"], axis=1), train.target)\n",
    "#train[\"target\"] = y\n",
    "#sns.countplot(x=train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "imposed-deployment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x249a8e7f948>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(train.corr().T.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-touch",
   "metadata": {},
   "source": [
    "Next, we will merge train and test to a dataframe that we will use to preprocess both simultaneously. We save the train and test ids so we will be able to reconstruct train and test later by subsetting full. We also need the ids for the submission to Kaggle. We also save and drop the target from the full dataset, as it is only present in the train set. We also drop the ID column because we will not need it at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = train[\"ID_code\"]\n",
    "test_id = test[\"ID_code\"]\n",
    "y_train = train.target.values\n",
    "full = pd.concat([train, test])\n",
    "full = full.drop([\"target\", \"ID_code\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "forward-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_0   var_1    var_2   var_3    var_4   var_5   var_6    var_7   var_8  \\\n",
       "0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187  18.6266 -4.9200   \n",
       "1  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208  16.5338  3.1468   \n",
       "2   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427  14.6155 -4.9193   \n",
       "3  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428  14.9250 -5.8609   \n",
       "4   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405  19.2514  6.2654   \n",
       "\n",
       "    var_9  ...  var_190  var_191  var_192  var_193  var_194  var_195  var_196  \\\n",
       "0  5.7470  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   7.8784   \n",
       "1  8.0851  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   8.1267   \n",
       "2  5.9525  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417  -6.5213   \n",
       "3  8.2450  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706  -2.9275   \n",
       "4  7.6784  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   3.9267   \n",
       "\n",
       "   var_197  var_198  var_199  \n",
       "0   8.5635  12.7803  -1.0914  \n",
       "1   8.7889  18.3560   1.9518  \n",
       "2   8.2675  14.7222   0.3965  \n",
       "3  10.2922  17.9697  -8.9996  \n",
       "4   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-semester",
   "metadata": {},
   "source": [
    "Next, we will check for missing values. There seem to be no missing values and nothing seems to be described on the Kaggle page about missing values, therefore, we will assume there are none at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "delayed-donor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(full.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-prefix",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop([\"target\"], axis=1).values\n",
    "y_train = train.target.values\n",
    "X_test = test.drop([\"ID_code\"], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "precise-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = full[:len(train_id)]\n",
    "X_test = full[len(train_id):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-theta",
   "metadata": {},
   "source": [
    "### OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acoustic-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wouter\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Wouter\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Wouter\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.911689999870584"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1 = 0.911689999870584 # No FE\n",
    "score2 = 0.7875982554476241 # After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efficient-spouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wouter\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Wouter\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\Wouter\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.911689999870584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LogisticRegression()\n",
    "cross_val_score(reg, X_train, y_train, cv=3, scoring=\"accuracy\").mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
